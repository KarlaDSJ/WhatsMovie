{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de los modelos para generar lenguaje\n",
    "\n",
    "Para probar el entrenamiento de cada modelo  utilizamos el conjunto de prueba de la siguiente manera: cada oración que lo conforma es dividida a la mitad, se le pasa al modelo la primera mitad y es concatenada con la respuesta, por último se compara la oración original con la que acabamos de crear para ver que tanto cambiaron, claro que esto tiene un problema existen dialogos que únicamente tienen una palabra, en estos casos en realidad no tenemos con que comprar (cosa que se modifica para la red seq to seq)\n",
    "\n",
    "Lo anterior aplica para el modelo de Bengio y RNN, para el último modelo el corpus de prueba es distinto, la idea es la misma pero aquí si tenemos la oración que siguiente\n",
    "\n",
    "Inicialmente pensamos en utilizar la métrica de Levenshtein, pues esta nos diría qué tan distantes son las oraciones verificando los símbolos en ellas, pero esto puede calificar mal inicios de oraciones como \"como estás\" y \"como te fue\", aunque ambas son respuestas válidas, sin embargo nuestro corpus también contiene combinaciones de palabras específicas, como \"si señor\", \"Torre Stark\" y \"Dr Strange\", lo que sería calificado positivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Genera bigramas dada una cadena\n",
    "    \"\"\"\n",
    "    words = sentence.split(' ')\n",
    "    return list(zip(words,words[1:]))\n",
    "\n",
    "def evaluate(s1, s2):\n",
    "    \"\"\"\n",
    "    Evalua que tan diferentes son 2 cadenas\n",
    "    Primero compara si alguna es sub cadena de otra,\n",
    "    si eso no pasa evalua si existen digramas compartidos\n",
    "    y hace un promedio para asignarle un valor entre 0 - 1\n",
    "    donde 0 indica que las cadenas no s eparecen en nada \n",
    "    1 que son subcadenas\n",
    "    \"\"\"\n",
    "    # Si una es subcadena de la otra, damos la calificacion mas alta: 1\n",
    "    if s1 in s2 or s2 in s1:\n",
    "        return 1\n",
    "    # Ahora intentamos con pares\n",
    "    bigrams = get_bigrams(s1)\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "    num_bigrams_in = 0\n",
    "    for bigram in bigrams:\n",
    "        if ' '.join(bigram) in s2:\n",
    "            num_bigrams_in += 1\n",
    "    return num_bigrams_in / len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('Tony stark, que tal tu dia?', 'Tony stark, que me cuentas?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los modelos y el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open('./pickles/datasets/test.pkl','rb'))\n",
    "predict_rnn = pickle.load(open('./pickles/predict/rnn.pkl','rb'))\n",
    "predict_bengio = pickle.load(open('./pickles/predict/bengio.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rnn.insert(2, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evals(test, predict):\n",
    "    evals = []\n",
    "    limit = len(test)\n",
    "    newlimit = 0\n",
    "    try:\n",
    "        for i in range(limit - 1):\n",
    "            evals.append(evaluate(test[i], predict[i]))\n",
    "            newlimit = i\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    xpoints = np.arange(0, newlimit+1, 1, dtype=int)\n",
    "    ypoints = np.array(evals)\n",
    "\n",
    "    plt.plot(xpoints, ypoints)\n",
    "    plt.show()\n",
    "\n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_evals = plot_evals(test, predict_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengio_evals = plot_evals(test, predict_bengio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, score in enumerate(bengio_evals):\n",
    "    if score != 1 and len(test[i].split(' ')) > 1:\n",
    "        print(test[i])\n",
    "        print(predict_bengio[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además recordemos que en la práctica 2 se hace el  análisis para la red de Bengio dando los siguientes resultados:\n",
    "\n",
    "*Para el entrenamiento con el 10% de las palabras tenemos:*\n",
    "```\n",
    "Entropy: 9.29426577135724\n",
    "Perplexity: 627.845498555216\n",
    "```\n",
    "\n",
    "*Para el entrenamiento con todo el corpus:*\n",
    "```\n",
    "Entropy: 12.874982418175305\n",
    "Perplexity: 7512.005574123771\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacion del modelo Sequence to sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra red Sequence to Sequence si esta entrenada para que, dada una oración de entrada o petición, devuelva una oración de salida o respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar estos modelos, vamos a calcular la entropía resultante con el set de prueba, mostrando la gráfica de como la entropia se ajusta con cada valor de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = pickle.load(open('./pickles/seqtoseq/test_pairs.pkl','rb'))\n",
    "test_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(test_data):\n",
    "    '''\n",
    "    Obtenemos la entropia promedio del modelo\n",
    "\n",
    "    Args:\n",
    "        test_data (list): conjunto de prueba\n",
    "    '''\n",
    "    H = 0.0\n",
    "    Hs = []\n",
    "    # calculamos entropia como el promedio de las probabilidades de cada oración\n",
    "    for before, after in tqdm(test_data):\n",
    "        inp = before_vectorization([before])\n",
    "        _, state = encoder(inp)\n",
    "        #Probabilidad de la cadena\n",
    "        p_cad = 1\n",
    "        for word in after.split(' '):\n",
    "            dec_inp = after_vectorization([word])[:, :1]\n",
    "            pred, state = decoder(dec_inp, state, training=False)\n",
    "            pred = tf.nn.softmax(pred)\n",
    "            p_cad *= pred[0][0][0].numpy()\n",
    "        #Longitud de la cadena\n",
    "        M = len(after.split(' '))\n",
    "        #Obtenemos la entropía cruzada de la cadena\n",
    "        if p_cad != 0:\n",
    "            H -= (1./M)*(np.log(p_cad)/np.log(2))\n",
    "        Hs.append(H)\n",
    "\n",
    "    return H/len(test_data), Hs\n",
    "\n",
    "entropy, history = get_entropy(test_pairs)\n",
    "perplexity = 2**entropy\n",
    "print(f'Entropy: {entropy}')\n",
    "print(f'Perplexity: {perplexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores obtenidos fueron:\n",
    "```\n",
    "Entropy: 40.61154408156754\n",
    "Perplexity: 1679936781077.174\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para graficar el histórico de la entropía, dividimos entre el número de oraciones procesadas en ese momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(history)), list(map(lambda h, i: h/i, history, range(1,len(test_pairs)+1))))\n",
    "plt.title('Entropy history')\n",
    "plt.ylabel('entropy')\n",
    "plt.xlabel('sentence index')\n",
    "plt.savefig('entropy_seqtoseq.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/entropy_seqtoseq.png\" alt=\"Red Bengio\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "Mientras que las redes de Bengio y RNN fueron las primeras en implementarse, la realidad es que no estaban enfocadas al problema que queríamos resolver. La red Sequence to Sequence fue la que mejores resultados nos arrojó en términos de coherencia y semántica, puesto que estaba entrenada para responder a un cierto input.\n",
    "\n",
    "Otras opciones que tenemos para mejorar el preprocesamiento del corpus omitiendo las palabras que no tengan valor real para las oraciones generadas (como \"yeah\"), o incluso cambiar el enfoque, como usar un Transformer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "602bbe8e8fe3127dc2d2bb8f57c49cfbb507be2cc0e090f10eb4c4d0af354b8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
