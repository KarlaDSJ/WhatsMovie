{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split #particiones\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparamos el dataset\n",
    "\n",
    "Para esta red se limpio y tokeniza el corpus pero dejamos signos de puntuación, además generamos pares de diálogos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    \"\"\"\n",
    "    Genera pares de dialogos\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    onlyfiles = []\n",
    "    paths = ['../corpus/Marvel', '../corpus/Christopher Nolan']\n",
    "    for path in paths:\n",
    "        onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    \n",
    "        for file in onlyfiles:\n",
    "            with open(path+\"/\"+file, 'rb') as f:\n",
    "                corpus = f.read().decode('utf-8', 'replace')\n",
    "                # quitamos #NAME? y texto descriptivo\n",
    "                corpus = re.sub(\"#NAME\\?|\\[.*\\]\", \"\", corpus)\n",
    "                # Rompemos dialogos por saltos de linea y giones\n",
    "                corpus = re.split(\"\\n|\\s-\\s\", corpus)\n",
    "                # quitamos espacios extra\n",
    "                corpus = list(map(lambda x: x.strip(), corpus))\n",
    "                # quitamos lineas vacias\n",
    "                corpus = list(filter(lambda x: x != '', corpus))\n",
    "                # formamos pares\n",
    "                pairs += list(zip(corpus, list(map(lambda x: '[start] '+x+' [end]', corpus[1:]))))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "text_pairs = prepare_dataset()\n",
    "text_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos en conjunto de entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_pairs, test_pairs = train_test_split(text_pairs, test_size=0.3)\n",
    "train_pairs, val_pairs = train_test_split(aux_pairs, test_size=0.1)\n",
    "\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos y cargamos los conjuntos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_pairs, open('./pickles/seqtoseq/train_pairs.pkl', 'wb'))\n",
    "pickle.dump(val_pairs, open('./pickles/seqtoseq/val_pairs.pkl', 'wb'))\n",
    "pickle.dump(test_pairs, open('./pickles/seqtoseq/test_pairs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = pickle.load(open('./pickles/seqtoseq/train_pairs.pkl','rb'))\n",
    "val_pairs = pickle.load(open('./pickles/seqtoseq/val_pairs.pkl','rb'))\n",
    "test_pairs = pickle.load(open('./pickles/seqtoseq/test_pairs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el tamaño del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(pairs):\n",
    "    corpus = ' '.join([sentence for pair in pairs for sentence in pair])\n",
    "    return len(Counter(corpus.split(' ')))\n",
    "\n",
    "get_vocab_size(text_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función para normalizar la entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "strip_chars\n",
    "\n",
    "vocab_size = 29948\n",
    "sequence_length = 10\n",
    "batch_size = 32\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    # primero pasamos la cadena a minúsculas\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    # luego le quitamos signos de puntuación y admiración\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en este caso tenemos 2 redes (encoder y decoder) necesitamos 2 diferentes capas para vectorizar, es decir que el texto de entrada se limpia y se convierte en un vector que lo representa, además se adapta esta capa al conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", \n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "after_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_before_texts = [pair[0] for pair in train_pairs]\n",
    "train_after_texts = [pair[1] for pair in train_pairs]\n",
    "before_vectorization.adapt(train_before_texts)\n",
    "after_vectorization.adapt(train_after_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_vectorization([['Good God'], ['Tony Stark']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los vectores y los bloques de datos para entrenar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, spa):\n",
    "    \"\"\"\n",
    "    Los pares que dialogós que inicialmente teníamos los repartimos \n",
    "    eng - el primer diálogo\n",
    "    spa - el diálogo de respuesta\n",
    "    \"\"\"\n",
    "    eng = before_vectorization(eng)\n",
    "    spa = after_vectorization(spa)\n",
    "    return eng, spa\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    \"\"\"\n",
    "    Bloques de datos aleatorios\n",
    "    \"\"\"\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(AUTOTUNE).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(inputs[0], targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "model_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Generamos la primera red, la que toma la primera oración y la codifica para ser la entrada de la segunda red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, voc_size, emb_dim, model_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(voc_size,\n",
    "                                                   emb_dim)\n",
    "        self.gru = tf.keras.layers.GRU(model_dim,\n",
    "                                       return_sequences=False,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, state=None):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.gru(x, initial_state=state)\n",
    "        return x, state\n",
    "    \n",
    "    \n",
    "encoder = Encoder(before_vectorization.vocabulary_size(),\n",
    "                  emb_dim, model_dim)\n",
    "output, enc_state = encoder(inputs)\n",
    "enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[:, :1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Generamos la segunda red, la que toma el vector de salida y genera el vector que pertenece al diálogo de respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, voc_size, emb_dim, model_dim):\n",
    "        super().__init__(self)\n",
    "        self.embedding = layers.Embedding(voc_size, emb_dim)\n",
    "        self.gru = layers.GRU(model_dim,\n",
    "                              return_sequences=True,\n",
    "                              return_state=True)\n",
    "        self.logits = layers.Dense(voc_size)\n",
    "\n",
    "    def call(self, x, state, training=False):\n",
    "        x = self.embedding(x, training=training)\n",
    "        x, state = self.gru(x, initial_state=state, training=training)\n",
    "        x = self.logits(x, training=training)\n",
    "\n",
    "        return x, state\n",
    "\n",
    "\n",
    "decoder = Decoder(voc_size=after_vectorization.vocabulary_size(),\n",
    "                  emb_dim=emb_dim,\n",
    "                  model_dim=model_dim)\n",
    "\n",
    "decoder(targets[:, :1], enc_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, state = encoder(inputs)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_avg = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss_avg = tf.keras.metrics.Mean(name='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(inputs[:3], targets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp_batch, tar_batch):\n",
    "    loss = tf.constant(0.0)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, state = encoder(inp_batch, training=True)\n",
    "\n",
    "        for step in range(0, tar_batch.shape[1] - 1):\n",
    "            dec_inp = tf.expand_dims(tar_batch[:, step], 1)\n",
    "            pred, state = decoder(dec_inp, state, \n",
    "                                  training=True)\n",
    "            loss += loss_function(tar_batch[:, step + 1], pred)\n",
    "        total_loss = loss / tar_batch.shape[1]\n",
    "    weights = encoder.trainable_weights + decoder.trainable_weights\n",
    "    gradients = tape.gradient(total_loss, weights)   \n",
    "    opt.apply_gradients(zip(gradients, weights))\n",
    "    train_loss_avg(total_loss)\n",
    "\n",
    "@tf.function\n",
    "def test_step(inp_batch, tar_batch):\n",
    "    loss = tf.constant(0.0)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, state = encoder(inp_batch, training=True)\n",
    "\n",
    "        for step in range(0, tar_batch.shape[1] - 1):\n",
    "            dec_inp = tf.expand_dims(tar_batch[:, step], 1)\n",
    "            pred, state = decoder(dec_inp, state, \n",
    "                                  training=True)\n",
    "            loss += loss_function(tar_batch[:, step + 1], pred)\n",
    "        total_loss = loss / tar_batch.shape[1]\n",
    "    \n",
    "    val_loss_avg(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_encoder = tf.train.Checkpoint(encoder)\n",
    "ckpt_encoder_manager = tf.train.CheckpointManager(\n",
    "    ckpt_encoder, \n",
    "    directory=\"./seqtoseqModelCheckpoint/encoder/\", \n",
    "    max_to_keep=1\n",
    ")\n",
    "\n",
    "ckpt_decoder = tf.train.Checkpoint(decoder)\n",
    "ckpt_decoder_manager = tf.train.CheckpointManager(\n",
    "    ckpt_decoder, \n",
    "    directory=\"./seqtoseqModelCheckpoint/decoder/\", \n",
    "    max_to_keep=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for text, target in train_ds:\n",
    "        train_step(text, target)\n",
    "        \n",
    "    print(f'Epoch: {epoch} Train loss: {train_loss_avg.result().numpy()}')\n",
    "    train_loss_history.append(train_loss_avg.result().numpy())\n",
    "    train_loss_avg.reset_states()\n",
    "    ckpt_encoder_manager.save(int(epoch))\n",
    "    ckpt_decoder_manager.save(int(epoch))\n",
    "    \n",
    "    for text, target in val_ds:\n",
    "        test_step(text, target)\n",
    "        \n",
    "    print(f'Val loss: {val_loss_avg.result().numpy()}')\n",
    "    val_loss_history.append(val_loss_avg.result().numpy())\n",
    "    val_loss_avg.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history)\n",
    "plt.plot(val_loss_history)\n",
    "plt.title('train and validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('training_validation_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_vocab = after_vectorization.get_vocabulary()\n",
    "after_index_lookup = dict(zip(range(len(after_vocab)), after_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = before_vectorization(['i love my dog'])\n",
    "_, state = encoder(inp)\n",
    "dec_inp = after_vectorization(['[start]'])[:, :1]\n",
    "output = []\n",
    "pred_index = ''\n",
    "\n",
    "while pred_index != '[end]':\n",
    "    pred, state = decoder(dec_inp, state, training=False)\n",
    "    dec_inp = tf.argmax(pred, axis=-1)\n",
    "    pred_index = after_index_lookup[dec_inp[0][0].numpy()]\n",
    "    output.append(pred_index)\n",
    "    \n",
    "' '.join(output[:-1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "602bbe8e8fe3127dc2d2bb8f57c49cfbb507be2cc0e090f10eb4c4d0af354b8b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
