{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKPFxhTOvkUc"
      },
      "source": [
        "# Proyecto final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVLEUX4UvkVV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "from tensorflow.keras import layers\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split #particiones\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_txt(path):\n",
        "    \"\"\"\n",
        "    Regresa una lista con el contenido de todos los archivos de un directorio\n",
        "\n",
        "    Args:\n",
        "        path (str): ruta de la carpeta\n",
        "    \"\"\"\n",
        "    text = []\n",
        "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "    \n",
        "    for file in onlyfiles:\n",
        "        with open(path+\"/\"+file, 'r') as f:\n",
        "            text += f.readlines()\n",
        "    return text\n",
        "\n",
        "# Guardamos cada película en un diccionario\n",
        "# cada entrada del diccionario es una lista con las peliculas leídas\n",
        "corpus = []\n",
        "corpus += get_txt(\"../corpus/Pride & Prejudice\")\n",
        "corpus += get_txt(\"../corpus/Marvel\")\n",
        "corpus += get_txt(\"../corpus/Christopher Nolan\")\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos el tamaño de la oracion mas grande\n",
        "\n",
        "def get_longest_sentence(corpus):\n",
        "    largest = []\n",
        "    for sentence in corpus:\n",
        "        if len(sentence) > len(largest):\n",
        "            largest = sentence\n",
        "    return largest\n",
        "\n",
        "largest = get_longest_sentence(corpus)\n",
        "print(largest)\n",
        "print(len(largest.split(' ')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Particiones \n",
        "train, test = train_test_split(corpus, test_size=0.3)\n",
        "print('Número de cadenas train:',len(train))\n",
        "print('Número de cadenas test:',len(test))\n",
        "pickle.dump(train, open('./pickles/datasets/train.pkl','wb'))\n",
        "pickle.dump(test, open('./pickles/datasets/test.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pickle.load(open('./pickles/datasets/train.pkl','rb'))\n",
        "test = pickle.load(open('./pickles/datasets/test.pkl','rb'))\n",
        "\n",
        "print('Número de cadenas train:',len(train))\n",
        "print('Número de cadenas test:',len(test))\n",
        "print(train[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MELoXMkwvkVh",
        "outputId": "a29c5e14-f919-4b70-e28e-8c7a3122d470"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "movies_tokens =  tokenizer.tokenize([' '.join(train)]).to_list()[0]\n",
        "movies_tokens[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT7WnB-ovkVi"
      },
      "outputs": [],
      "source": [
        "words_ds = tf.data.Dataset.from_tensor_slices(movies_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lZV_AVvkVj",
        "outputId": "4271ad9f-737c-448a-c4f3-152ca611b9ff"
      },
      "outputs": [],
      "source": [
        "for words in words_ds.take(20):\n",
        "    print(words.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYvr2BrovkVm",
        "outputId": "3d294417-a629-4195-b702-9ad427a32230"
      },
      "outputs": [],
      "source": [
        "seq_length = 21\n",
        "words_batches = words_ds.batch(seq_length+1, \n",
        "                               drop_remainder=True)\n",
        "\n",
        "for words in words_batches.take(1):\n",
        "    print(words.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "words_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZazD_0dvkVr"
      },
      "outputs": [],
      "source": [
        "def join_strings(tokens):\n",
        "    return tf.strings.reduce_join(tokens, axis=0, separator=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "words_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_train_ds = words_batches.map(join_strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOZVoc3nvkVs"
      },
      "outputs": [],
      "source": [
        "raw_train_ds = tf.data.Dataset.from_tensor_slices(np.array(train).flatten())\n",
        "batch_size = 32\n",
        "BUFFER_SIZE = len(raw_train_ds)\n",
        "\n",
        "raw_train_ds = (\n",
        "    raw_train_ds\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(batch_size, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz-rLb6ivkVu",
        "outputId": "fb01a71d-cd35-48c8-c2a5-13af6e2ee94b"
      },
      "outputs": [],
      "source": [
        "for batch in raw_train_ds.take(1):\n",
        "    print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k995ah94vkVv",
        "outputId": "3eaeabd8-d6d4-4f97-a160-549dee7d3451"
      },
      "outputs": [],
      "source": [
        "voc_size = 20406\n",
        "\n",
        "def clean_text(raw_text):\n",
        "    #lowercase = tf.strings.lower(raw_text)\n",
        "    #trim = tf.strings.strip(lowercase)\n",
        "    #clean = tf.strings.regex_replace(trim, '\\n', ' ')\n",
        "    clean = tf.strings.unicode_decode(raw_text, 'utf-8', errors='ignore')\n",
        "    return clean\n",
        "\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=clean_text,\n",
        "    max_tokens=voc_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=21,\n",
        "    #split='character'\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(raw_train_ds, 32)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "voc_size = len(vocab)\n",
        "voc_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPJo3S79vkV1",
        "outputId": "3f7dd02a-e20b-44ee-9d65-583ac3f5e90f"
      },
      "outputs": [],
      "source": [
        "vectorize_layer(['Love you', '3 millions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5Nf5MUJvkV3"
      },
      "outputs": [],
      "source": [
        "def get_input_target(text):\n",
        "    tokenized_text = vectorize_layer(text)\n",
        "    input_text = tokenized_text[:, :-1]\n",
        "    target_text = tokenized_text[:, 1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l35I8kiDvkV4"
      },
      "outputs": [],
      "source": [
        "train_ds = raw_train_ds.map(get_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo9Y6BcuvkV8",
        "outputId": "34a9d8f3-b02b-4ed9-a527-1f6a5cd63e2b"
      },
      "outputs": [],
      "source": [
        "for input_batch, target_batch in train_ds.take(1):\n",
        "    print(input_batch.shape, target_batch.shape)\n",
        "    print(input_batch[0], target_batch[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_ougyUNvkV-"
      },
      "source": [
        "Definir modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iYC0TAMvkV-"
      },
      "outputs": [],
      "source": [
        "emb_dim = 256\n",
        "model_dim = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHh6zDX8vkV_"
      },
      "outputs": [],
      "source": [
        "class RNN(tf.keras.Model):\n",
        "    def __init__(self, voc_size, emb_dim, model_dim):\n",
        "        super().__init__(self)\n",
        "        self.embedding = layers.Embedding(voc_size, emb_dim)\n",
        "        self.gru = layers.GRU(model_dim,\n",
        "                              return_sequences=True,\n",
        "                              return_state=True)\n",
        "        self.logits = layers.Dense(voc_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.logits(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x \n",
        "\n",
        "model = RNN(voc_size=voc_size,\n",
        "            emb_dim=emb_dim,\n",
        "            model_dim=model_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "CgpJHZRCvkWH",
        "outputId": "f03f72ed-2655-4a18-cddc-93deb39a6fd9"
      },
      "outputs": [],
      "source": [
        "for input_batch, target_batch in train_ds.take(1):\n",
        "    predictions = model(target_batch)\n",
        "    print(predictions.shape, target_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcQO3mmpvkWI",
        "outputId": "f0f36fce-d931-407e-9a6e-ed3b8ba2622d"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQj-8On2vkWJ"
      },
      "outputs": [],
      "source": [
        "predictions[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyk1OErIvkWJ"
      },
      "outputs": [],
      "source": [
        "pred_indices = tf.random.categorical(predictions[0], num_samples=1)\n",
        "pred_indices[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys5tDgt6vkWK"
      },
      "source": [
        "Obtener palabras a travez de indices con vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS8UyYDbvkWL"
      },
      "outputs": [],
      "source": [
        "' '.join([vocab[_] for _ in input_batch[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_Jp1fydvkWM"
      },
      "outputs": [],
      "source": [
        "' '.join([vocab[_] for _ in pred_indices[:, 0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAX3XpNMvkWN"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fKaiU7cvkWN"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "loss_metric = tf.keras.metrics.Mean(name='loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnJDRdulvkWO"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(input_batch, target_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(input_batch, training=True)\n",
        "        loss_value = loss(target_batch, logits)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "    opt.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "    loss_metric(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3L3-LZdvkWP"
      },
      "outputs": [],
      "source": [
        "epochs = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA4gdMhvvkWQ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    for input_batch, target_batch in train_ds:\n",
        "        train_step(input_batch, target_batch)\n",
        "        \n",
        "    print(f'Epoch: {epoch} Loss: {loss_metric.result().numpy()}')\n",
        "    loss_metric.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuyBKXD2vkWV"
      },
      "source": [
        "# Guardamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es_CCnwjvkWX"
      },
      "outputs": [],
      "source": [
        "model.save('./modelTensor/model_RNN.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kE8Oi_VvkWY"
      },
      "outputs": [],
      "source": [
        "model2 = tf.keras.models.load_model('./modelTensor/model_RNN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-TrUjpIvkWZ"
      },
      "source": [
        "# Generación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjzI_h9JvkWa"
      },
      "outputs": [],
      "source": [
        "states = None\n",
        "start = 'tony stark'\n",
        "context = tf.constant([start])\n",
        "output = [start]\n",
        "\n",
        "for i in range(50):\n",
        "    #print(vectorize_layer(context)[:, :1])\n",
        "    # Obtener solo el primer elemento que regresa vectorize_layer\n",
        "    pred_logits, states = model(vectorize_layer(context)[:, :1], \n",
        "                                states=states, return_state=True)\n",
        "    #print(pred_logits.shape)\n",
        "    pred_index = tf.random.categorical(pred_logits[:, -1, :], \n",
        "                                       num_samples=1)\n",
        "\n",
        "    #print(vocab[pred_index[0, 0]])\n",
        "    context = tf.constant([vocab[pred_index[0, 0]]])\n",
        "    output.append(vocab[pred_index[0, 0]])\n",
        "    \n",
        "' '.join(output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ProyectoFinal.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "602bbe8e8fe3127dc2d2bb8f57c49cfbb507be2cc0e090f10eb4c4d0af354b8b"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
