{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKPFxhTOvkUc"
      },
      "source": [
        "# Proyecto final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oVLEUX4UvkVV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "from tensorflow.keras import layers\n",
        "import pickle\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_d9nkSFfvkVe",
        "outputId": "20d2b5e4-dcf9-47c0-e476-564385a0ad84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1 truth universally acknowledged single man possession good fortune must want wife however little kn'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = pickle.load(open('./corpus.pkl', 'rb'))\n",
        "corpus = [x for s in corpus for x in s]\n",
        "corpus = ' '.join(corpus)\n",
        "corpus[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MELoXMkwvkVh",
        "outputId": "a29c5e14-f919-4b70-e28e-8c7a3122d470"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[b'1',\n",
              " b'truth',\n",
              " b'universally',\n",
              " b'acknowledged',\n",
              " b'single',\n",
              " b'man',\n",
              " b'possession',\n",
              " b'good',\n",
              " b'fortune',\n",
              " b'must']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "movies_tokens =  tokenizer.tokenize([corpus]).to_list()[0]\n",
        "movies_tokens[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jT7WnB-ovkVi"
      },
      "outputs": [],
      "source": [
        "words_ds = tf.data.Dataset.from_tensor_slices(movies_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lZV_AVvkVj",
        "outputId": "4271ad9f-737c-448a-c4f3-152ca611b9ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'1'\n",
            "b'truth'\n",
            "b'universally'\n",
            "b'acknowledged'\n",
            "b'single'\n",
            "b'man'\n",
            "b'possession'\n",
            "b'good'\n",
            "b'fortune'\n",
            "b'must'\n",
            "b'want'\n",
            "b'wife'\n",
            "b'however'\n",
            "b'little'\n",
            "b'known'\n",
            "b'feelings'\n",
            "b'views'\n",
            "b'man'\n",
            "b'may'\n",
            "b'first'\n"
          ]
        }
      ],
      "source": [
        "for words in words_ds.take(20):\n",
        "    print(words.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYvr2BrovkVm",
        "outputId": "3d294417-a629-4195-b702-9ad427a32230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[b'1' b'truth' b'universally' b'acknowledged' b'single' b'man'\n",
            " b'possession' b'good' b'fortune' b'must' b'want' b'wife' b'however'\n",
            " b'little' b'known' b'feelings' b'views' b'man' b'may' b'first'\n",
            " b'entering' b'neighbourhood' b'truth' b'well' b'fixed' b'minds'\n",
            " b'surrounding' b'families' b'considered' b'rightful' b'property' b'one'\n",
            " b'daughters' b'dear' b'mr' b'bennet' b'said' b'lady' b'one' b'day'\n",
            " b'heard' b'netherfield' b'park' b'let' b'last' b'mr' b'bennet' b'replied'\n",
            " b'returned' b'mrs' b'long']\n"
          ]
        }
      ],
      "source": [
        "seq_length = 50\n",
        "words_batches = words_ds.batch(seq_length+1, \n",
        "                               drop_remainder=True)\n",
        "\n",
        "for words in words_batches.take(1):\n",
        "    print(words.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mZazD_0dvkVr"
      },
      "outputs": [],
      "source": [
        "def join_strings(tokens):\n",
        "    return tf.strings.reduce_join(tokens, axis=0, separator=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bOZVoc3nvkVs"
      },
      "outputs": [],
      "source": [
        "raw_train_ds = words_batches.map(join_strings)\n",
        "batch_size = 32\n",
        "BUFFER_SIZE = len(raw_train_ds)\n",
        "\n",
        "raw_train_ds = (\n",
        "    raw_train_ds\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(batch_size, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz-rLb6ivkVu",
        "outputId": "fb01a71d-cd35-48c8-c2a5-13af6e2ee94b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'safe difficult times us must stand together good asgard yes course good wait word may beg indulgence majesty perhaps reconsider done scoffs need horse nt horses dogs cats birds give one large enough ride honking still need lift never done anything like ever done anything like many times brave well stole'\n",
            " b'got got first like said ca nt stay groans move right behind pepper get gonna find way around stop stopping get get outside go pepper straining repulsor fires scream groaning grunting ch god tony groaning straining jarvis sir ms potts clear structure straining machine gun firing grunting groans mechanical whirring gasps'\n",
            " b'could shut ned hi captain america whether classroom battlefield know met stole shield today good friend gym teacher conducting captain america fitness challenge thank captain pretty sure war criminal show videos required state let avengers pay taxes hulk smell like bet smells nice shut captain america cool like mean old grandpa'\n",
            " b'nt childishly cold mockingly tell know tell connected worth shot coughs groaning saleswoman get affordable gorgeous goes decor happened picture woman 1 go override technicians conversing happening man 1 fine override los angeles woman 2 trying nothing man 2 well feed coming man 3 nt know receivers entire east coast satellites'\n",
            " b'nt mal please nt children keep telling nt believe know wrong real keep telling know believe feel guilt feel guilt mal matter matter hopeless matter confused guilt always reminding truth truth idea caused question reality came planted idea mind talking reason knew inception possible first name lost knew needed escape would'\n",
            " b'hand dealt schmidt believes walks footsteps gods world satisfy realize nuts nt sanity plan consequence target target everywhere tomorrow hydra stand master world borne victory wings valkyrie enemies weapons powerless us shoot one plane hundreds rain fire upon cut one head two shall take place hail hydra hail hydra hail hydra'\n",
            " b'know got inside already mix could abomination gasping nt say unwilling need informed consent given trying explain nt know ladling clearly worked let assume nt understand word saying get back table fix abomination growling abomination laughing soldier 1 shoot soldier 2 soldiers screaming soldier 5 delta 4 leader took two guys'\n",
            " b'basic layout bookstore caf almost everything else ariadne people projections subconscious name remember dreamer build world subject mind populates literally talk subconscious one ways extract information subject else creating something secure like bank vault jail mind automatically fills information trying protect name well guess thought dream space would visual feel question'\n",
            " b'prototype mother worked together never got use realize working maybe time finished damn time cap would lot easier week ago call tony wo nt believe us even knows accords let help maybe know guy distant explosion strucker pa report stations immediately drill attack soldiers shouting indistinctly attack grunts powering yells grunting'\n",
            " b'peter peter peter gonna go get liz right pete look pale want something drink like bourbon scotch something like old enough drink right answer wow wow wow wow look beautiful please nt embarrass dad nt pete look really good right answer corsage thanks well chauffeur let get show road take pictures'\n",
            " b'stand sam ca nt shake guy right behind sirens wailing black panther grunting tyres screeching sirens continue wailing stand congratulations cap criminal shouting indistinctly highness pinch paprika pinch soft music playing stereo paprikash thought might lift spirits wanda chuckles spirits lifted defense nt actually ever eaten anything may please wanda hmm'\n",
            " b'elizabeth ran sister seeing well guarded cold attended drawingroom welcomed two friends many professions pleasure elizabeth never seen agreeable hour passed gentlemen appeared powers conversation considerable could describe entertainment accuracy relate anecdote humour laugh acquaintance spirit gentlemen entered jane longer first object bingley eyes instantly turned toward darcy something say advanced'\n",
            " b'right pickles smush real flat thanks got boss aunt right ten dollars 5 comment 10 come joking joking 5 murph buddy school ah know boring got better things stay school kid stay school otherwise gonna end like great best sandwiches queens finally excuse matter could hold second thanks anybody bike buddy'\n",
            " b'frequently invited week know mother ideas necessity constant company friends really upon honour try think wisest hope satisfied aunt assured elizabeth thanked kindness hints parted wonderful instance advice given point without resented mr collins returned hertfordshire soon quitted gardiners jane took abode lucases arrival great inconvenience mrs bennet marriage fast approaching'\n",
            " b'daughter groot hell nt got long lifespan anyway standing yall happy standing bunch jackasses standing circle stone reacts anything organic bigger target bigger power surge ronan got touch stone planet surface zap plants animals nova corps everything die ronan make surface rocket lead team blow hole dark aster starboard hull craft'\n",
            " b'send loki soup yes milady ask librarians pull volumes astronomy shelf fancy broad mother dies today today ca nt ca nt nt nt come bad idea come think panic attack come right nt bad think one lost people think lost family ever quill groot drax chick antenna gone get mom gone'\n",
            " b'entire life work nt really much left lose clever far clever anyone else realm realm realm think strange good strange bad strange quite sure yet tires screeching laughing sorry really see soon enough promised answers seek bridge like einsteinrosen bridge like rainbow bridge god hope crazy loki softly never get used'\n",
            " b'unhappy gonna try keep way match cap panting thanks barton gasps people chattering excitedly know cradle power make real change terrifies would nt call comfort stop got window four three give hell always picking boys heading overpass got shot romanoff way hard right people exclaiming tyres squealing grunting engine revving way'\n",
            " b'2008 remote russian missile station overwhelmed held week station retaken 241 one warhead three quarters kilo lighter missing 241 surfaced opera house siege kiev 14 th coming propose partnership would nt partner take care record someone arms trade training knows cover tracks shocking intelligence agent right burn hell andrei ca nt'\n",
            " b'kill think drop gun picked wrong side hope dad hank hank hank listen gonna right gonna fine take suit blow brains peel got 1033 pym tech request immediate backup go go go get roof radio ahead want make sure helicopter ready take two kill anything comes vault dad move need get'\n",
            " b'make yes yes come case get back doyle come get hatch go go go shit manually overriding inside hatch cooper wait cooper engines flooded gonna shut amelia yelling holy shit hang yelling case problem waterlogged let drain goddamn told leave nt told get ass back one us thinking mission thinking getting'\n",
            " b'fire groot fire prisoners return sleeping areas idiot supposed fight things without stuff animal control fire command creepy little beast need need good luck internally wired figure something drop leg drop leg move back cell rocket move watchtower man lain aaskavariian one time man need available guards full combat gear spare'\n",
            " b'thing neck show listen threatening sparkles deal wanna get back assplace assberg asgard contender defeats champion freedom shall win fine point direction whoever ass kick call contender direction would way lord loki take easy man pile rocks waving actually thing allow introduce name korg kind like leader made rocks see nt'\n",
            " b'swervin waves like line us move verse turf outta line put work draw line cross first need time need search like wine make worse skrrt skrrt vert skrrt skrrt ride land boeing jet make land slow motion dance eyes see trance run away prance show hideaway hide let blam ai nt'\n",
            " b'strong without hammer ca nt thor god hammers hammer help control power focus never source strength late already taken asgard asgard place never could asgard asgard people stand even right people need help strong stronger tell brother god late missing eye nt think disband revengers hit lightning blast hit biggest lightning'\n",
            " b'subtle bells whistles called badass fine right go top draw nt stay worst place got spot mine kill box tony go die see nice rhodey get wow think lead next time sorry boss use oneoff told five minutes ago guy excuse placed arrest kidding hands behind back sir trying help get'\n",
            " b'put tracker suit put everything suit including heater whoa better thanks thinking guy wings source weapons got ta take take huh crockett people handle sort thing avengers little pay grade mr stark nt come fine thank god place wifi would toast right thank ganesh cheers look forget flying vulture guy please'\n",
            " b'may express right proud true replied elizabeth could easily forgive pride mortified mine pride observed mary piqued upon solidity reflections common failing believe ever read convinced common indeed human nature particularly prone us cherish feeling selfcomplacency score quality real imaginary vanity pride different things though words often used synonymously person may'\n",
            " b'allow everything settled satisfaction entered house earnestly entreated name day make happiest men though solicitation must waived present lady felt inclination trifle happiness stupidity favoured nature must guard courtship charm could make woman wish continuance lucas accepted solely pure disinterested desire establishment cared soon establishment gained sir william lady lucas speedily'\n",
            " b'success certainly looked friend great deal expression look disputable earnest steadfast gaze often doubted whether much admiration sometimes seemed nothing absence mind twice suggested elizabeth possibility partial elizabeth always laughed idea mrs collins think right press subject danger raising expectations might end disappointment opinion admitted doubt friend dislike would vanish could'\n",
            " b'fire mordo london sanctum fallen new york one attacked twice know going next hong kong told fight like life depended one day might well today day defeat alone sanctum already fallen dark dimension dormammu coming late nothing stop necessarily spell working got second chance whoa wong breaking laws nature know well'\n",
            " b'nice workspace laura huh put baffling ca nt hear kids running around think guys always eat kitchen anyway barton one eats dining room people clamouring roaring nt lot time get ass boat panting big guy sun getting real low rogers safe gonna buckle secure gear find seat zrinka costel market costel'], shape=(32,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for batch in raw_train_ds.take(1):\n",
        "    print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k995ah94vkVv",
        "outputId": "3eaeabd8-d6d4-4f97-a160-549dee7d3451"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16354"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "voc_size = 16380\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=None,\n",
        "    max_tokens=voc_size - 1,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_length + 1,\n",
        "    #split='character'\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(raw_train_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "voc_size = len(vocab)\n",
        "voc_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPJo3S79vkV1",
        "outputId": "3f7dd02a-e20b-44ee-9d65-583ac3f5e90f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 51), dtype=int64, numpy=\n",
              "array([[   1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0],\n",
              "       [2728, 1852,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0]])>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorize_layer(['Love you', '3 millions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q5Nf5MUJvkV3"
      },
      "outputs": [],
      "source": [
        "def get_input_target(text):\n",
        "    tokenized_text = vectorize_layer(text)\n",
        "    input_text = tokenized_text[:, :-1]\n",
        "    target_text = tokenized_text[:, 1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l35I8kiDvkV4"
      },
      "outputs": [],
      "source": [
        "train_ds = raw_train_ds.map(get_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo9Y6BcuvkV8",
        "outputId": "34a9d8f3-b02b-4ed9-a527-1f6a5cd63e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 50) (32, 50)\n",
            "tf.Tensor(\n",
            "[  216    43    37     2   117   333  1145 13540  2540   767  1630    28\n",
            "    35   564   235   589   411   580    13  4528  1628   227   296  6251\n",
            "  4900  1392   361  4738  2316   503  4168   244 10038  1790   805   230\n",
            "  4964   391     9   140   875   349 14176   233  5573  2864   833     2\n",
            "  1306   281], shape=(50,), dtype=int64) tf.Tensor(\n",
            "[   43    37     2   117   333  1145 13540  2540   767  1630    28    35\n",
            "   564   235   589   411   580    13  4528  1628   227   296  6251  4900\n",
            "  1392   361  4738  2316   503  4168   244 10038  1790   805   230  4964\n",
            "   391     9   140   875   349 14176   233  5573  2864   833     2  1306\n",
            "   281  1650], shape=(50,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for input_batch, target_batch in train_ds.take(1):\n",
        "    print(input_batch.shape, target_batch.shape)\n",
        "    print(input_batch[0], target_batch[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_ougyUNvkV-"
      },
      "source": [
        "Definir modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9iYC0TAMvkV-"
      },
      "outputs": [],
      "source": [
        "emb_dim = 256\n",
        "model_dim = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pHh6zDX8vkV_"
      },
      "outputs": [],
      "source": [
        "class RNN(tf.keras.Model):\n",
        "    def __init__(self, voc_size, emb_dim, model_dim):\n",
        "        super().__init__(self)\n",
        "        self.embedding = layers.Embedding(voc_size, emb_dim)\n",
        "        self.gru = layers.GRU(model_dim,\n",
        "                              return_sequences=True,\n",
        "                              return_state=True)\n",
        "        self.logits = layers.Dense(voc_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.logits(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x \n",
        "\n",
        "model = RNN(voc_size=voc_size,\n",
        "            emb_dim=emb_dim,\n",
        "            model_dim=model_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "CgpJHZRCvkWH",
        "outputId": "f03f72ed-2655-4a18-cddc-93deb39a6fd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 50, 16354) (32, 50)\n"
          ]
        }
      ],
      "source": [
        "for input_batch, target_batch in train_ds.take(1):\n",
        "    predictions = model(target_batch)\n",
        "    print(predictions.shape, target_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcQO3mmpvkWI",
        "outputId": "f0f36fce-d931-407e-9a6e-ed3b8ba2622d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"rnn\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  4186624   \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  16762850  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,887,778\n",
            "Trainable params: 24,887,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VQj-8On2vkWJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([50, 16354])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Tyk1OErIvkWJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
              "array([ 9440, 12747,  7556, 11468, 14357, 15374,  9365,  4780,  1292,\n",
              "       11419, 14064,  8400,   353,  4579, 14653, 12278, 14563,  5589,\n",
              "         865,  5567, 16116,  7819,  6907,   460,  3867, 14386,  1099,\n",
              "        1451, 12952, 14009,  6716,  8379, 15562, 14306,  8918, 14580,\n",
              "       11460, 14858,  3575,  3011, 13421,  1332, 13786, 12325,  5628,\n",
              "       14996, 10755, 14855,  4454,  2481])>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_indices = tf.random.categorical(predictions[0], num_samples=1)\n",
        "pred_indices[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys5tDgt6vkWK"
      },
      "source": [
        "Obtener palabras a travez de indices con vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iS8UyYDbvkWL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ten years ago lazarus missions prof brand twelve possible worlds twelve ranger launches carrying bravest humans ever live led remarkable dr mann doyle person landing pod enough life support two years could use hibernation stretch making observations organics decade mission assess world showed potential could send signal bed long nap'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' '.join([vocab[_] for _ in input_batch[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1_Jp1fydvkWM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'willfully libertywhich roads puff duplicitous carpenter yahtzee western seat quim faithfully freddy knows disposal designto nada disappears analysis soul atom airduct pays anna full mortification drooping shouting anxiety jilt fibrillation contrived gaieties bosoms elbow clearer diminishes punched crib display stammering hes bitch fusing mountaintops 35 consumes shrouded crimefighting inconvenient central'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' '.join([vocab[_] for _ in pred_indices[:, 0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAX3XpNMvkWN"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0fKaiU7cvkWN"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "loss_metric = tf.keras.metrics.Mean(name='loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HnJDRdulvkWO"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(input_batch, target_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(input_batch, training=True)\n",
        "        loss_value = loss(target_batch, logits)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "    opt.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "    loss_metric(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "I3L3-LZdvkWP"
      },
      "outputs": [],
      "source": [
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA4gdMhvvkWQ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    for input_batch, target_batch in train_ds:\n",
        "        train_step(input_batch, target_batch)\n",
        "        \n",
        "    print(f'Epoch: {epoch} Loss: {loss_metric.result().numpy()}')\n",
        "    loss_metric.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuyBKXD2vkWV"
      },
      "source": [
        "# Guardamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es_CCnwjvkWX"
      },
      "outputs": [],
      "source": [
        "model.save('./modelTensor/model_RNN.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kE8Oi_VvkWY"
      },
      "outputs": [],
      "source": [
        "model2 = tf.keras.models.load_model('./modelTensor/model_RNN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-TrUjpIvkWZ"
      },
      "source": [
        "# Generación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjzI_h9JvkWa"
      },
      "outputs": [],
      "source": [
        "states = None\n",
        "start = 'tony stark'\n",
        "context = tf.constant([start])\n",
        "output = [start]\n",
        "\n",
        "for i in range(50):\n",
        "    #print(vectorize_layer(context)[:, :1])\n",
        "    # Obtener solo el primer elemento que regresa vectorize_layer\n",
        "    pred_logits, states = model(vectorize_layer(context)[:, :1], \n",
        "                                states=states, return_state=True)\n",
        "    #print(pred_logits.shape)\n",
        "    pred_index = tf.random.categorical(pred_logits[:, -1, :], \n",
        "                                       num_samples=1)\n",
        "\n",
        "    #print(vocab[pred_index[0, 0]])\n",
        "    context = tf.constant([vocab[pred_index[0, 0]]])\n",
        "    output.append(vocab[pred_index[0, 0]])\n",
        "    \n",
        "' '.join(output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ProyectoFinal.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
