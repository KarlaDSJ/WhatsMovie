{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKPFxhTOvkUc"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVLEUX4UvkVV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import re #regex\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split #particiones\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_txt(path):\n",
        "    \"\"\"\n",
        "    Regresa una lista con el contenido de todos los archivos de un directorio\n",
        "\n",
        "    Args:\n",
        "        path (str): ruta de la carpeta\n",
        "    \"\"\"\n",
        "    text = []\n",
        "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "    \n",
        "    for file in onlyfiles:\n",
        "        with open(path+\"/\"+file, 'r') as f:\n",
        "            text += f.readlines()\n",
        "    return text\n",
        "\n",
        "# Guardamos cada película en un diccionario\n",
        "# cada entrada del diccionario es una lista con las peliculas leídas\n",
        "corpus = []\n",
        "corpus += get_txt(\"../corpus/Pride & Prejudice\")\n",
        "corpus += get_txt(\"../corpus/Marvel\")\n",
        "corpus += get_txt(\"../corpus/Christopher Nolan\")\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_clean(corpus):\n",
        "    \"\"\"\n",
        "    Limpiamos el corpus pero manteniendo algunos signos de puntuación \n",
        "    que apaortan información\n",
        "\n",
        "    Args:\n",
        "        corpus(list): conjunto de diálogos de películas\n",
        "    \"\"\"\n",
        "    clean = []\n",
        "    pattern = r'[^a-z0-9 .,!;:]'\n",
        "    for w in corpus:\n",
        "        #convierte a minúsculas\n",
        "        w = w.lower()\n",
        "        w = re.sub(pattern,'', w)\n",
        "        if w != '' or w == '\\n':\n",
        "            clean.append(w.strip())\n",
        "    return clean\n",
        "\n",
        "clean = get_clean(corpus)\n",
        "clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_longest_sentence(corpus):\n",
        "    \"\"\"\n",
        "    Obtenemos la oración mas grande del corpus\n",
        "\n",
        "    Args:\n",
        "        corpus(list): conjunto de diálogos de películas\n",
        "    \"\"\"\n",
        "    largest = []\n",
        "    for sentence in corpus:\n",
        "        if len(sentence) > len(largest):\n",
        "            largest = sentence\n",
        "    return largest\n",
        "\n",
        "largest = get_longest_sentence(clean)\n",
        "print(largest)\n",
        "print(len(largest.split(' ')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cargamos el conjunto de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pickle.load(open('./pickles/datasets/train.pkl','rb'))\n",
        "print('Número de cadenas train:',len(train))\n",
        "print(train[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adecuamos los datos al tipo que requiere TensorFlow y preparamos los datos en bloques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOZVoc3nvkVs"
      },
      "outputs": [],
      "source": [
        "raw_train_ds = tf.data.Dataset.from_tensor_slices(np.array(train).flatten())\n",
        "batch_size = 32 #Tamaño del bloque\n",
        "BUFFER_SIZE = len(raw_train_ds)\n",
        "#Creamos bloques\n",
        "raw_train_ds = (\n",
        "    raw_train_ds\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(batch_size, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz-rLb6ivkVu",
        "outputId": "fb01a71d-cd35-48c8-c2a5-13af6e2ee94b"
      },
      "outputs": [],
      "source": [
        "# Verificamos que lo hace bien\n",
        "for batch in raw_train_ds.take(1):\n",
        "    print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hacemos que el texto de entrada se convierta en un vector que lo representa y adaptamos esta capa al conjunto de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k995ah94vkVv",
        "outputId": "3eaeabd8-d6d4-4f97-a160-549dee7d3451"
      },
      "outputs": [],
      "source": [
        "voc_size = 20406 #Tamaño del vocabulario, del corpus\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=None,\n",
        "    max_tokens=voc_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=22,\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(raw_train_ds, 32)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "voc_size = len(vocab)\n",
        "voc_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPJo3S79vkV1",
        "outputId": "3f7dd02a-e20b-44ee-9d65-583ac3f5e90f"
      },
      "outputs": [],
      "source": [
        "vectorize_layer(['Love you', '3 millions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q5Nf5MUJvkV3"
      },
      "outputs": [],
      "source": [
        "def get_input_target(text):\n",
        "    \"\"\"\n",
        "    Dada la representación de una cadena \n",
        "    desplazamos esa representación en uno \n",
        "    (capa oculta, conserva contexto).\n",
        "    Regresa el vector y el vector desplazado\n",
        "\n",
        "    Args:\n",
        "        text (str): frase a vectorizar\n",
        "    \"\"\"\n",
        "    tokenized_text = vectorize_layer(text)\n",
        "    input_text = tokenized_text[:, :-1]\n",
        "    target_text = tokenized_text[:, 1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtenemos los vectores con su contexto (capa oculta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo9Y6BcuvkV8",
        "outputId": "34a9d8f3-b02b-4ed9-a527-1f6a5cd63e2b"
      },
      "outputs": [],
      "source": [
        "train_ds = raw_train_ds.map(get_input_target)\n",
        "\n",
        "for input_batch, target_batch in train_ds.take(1):\n",
        "    print(input_batch.shape, target_batch.shape)\n",
        "    print(input_batch[0], target_batch[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_ougyUNvkV-"
      },
      "source": [
        "Definir modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emb_dim = 256\n",
        "model_dim = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRNN(tf.keras.Model):\n",
        "    def __init__(self, voc_size, emb_dim, model_dim):\n",
        "        super().__init__(self)\n",
        "        \"\"\"\n",
        "        Creamos las capas de la red \n",
        "        \"\"\"\n",
        "        self.embedding = layers.Embedding(voc_size, emb_dim)\n",
        "        self.gru = layers.GRU(model_dim,\n",
        "                              return_sequences=True,\n",
        "                              return_state=True)\n",
        "        self.logits = layers.Dense(voc_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        \"\"\"\n",
        "        Devuelve el valor del entrenamiento o entrena \n",
        "        \"\"\"\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.logits(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x \n",
        "\n",
        "#Creamos el modelo\n",
        "rnn = CustomRNN(voc_size=voc_size,\n",
        "            emb_dim=emb_dim,\n",
        "            model_dim=model_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Probamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for input_batch, target_batch in train_ds.take(1):\n",
        "    predictions = rnn(target_batch)\n",
        "    print(predictions.shape, target_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_indices = tf.random.categorical(predictions[0], num_samples=1)\n",
        "pred_indices[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "' '.join([vocab[_] for _ in input_batch[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAX3XpNMvkWN"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fKaiU7cvkWN"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "loss_metric = tf.keras.metrics.Mean(name='loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnJDRdulvkWO"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(input_batch, target_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = rnn(input_batch, training=True)\n",
        "        loss_value = loss(target_batch, logits)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, rnn.trainable_weights)\n",
        "    opt.apply_gradients(zip(gradients, rnn.trainable_weights))\n",
        "    loss_metric(loss_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos el modelo para guardar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt = tf.train.Checkpoint(rnn)\n",
        "ckpt_manager = tf.train.CheckpointManager(\n",
        "    ckpt, \n",
        "    directory=\"./rnnModelCheckpoint/\", \n",
        "    max_to_keep=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrenamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA4gdMhvvkWQ"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    for input_batch, target_batch in train_ds:\n",
        "        train_step(input_batch, target_batch)\n",
        "        \n",
        "    print(f'Epoch: {epoch} Loss: {loss_metric.result().numpy()}')\n",
        "    loss_metric.reset_states()\n",
        "    ckpt_manager.save(int(epoch)) #Guardamos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cargamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = CustomRNN(voc_size=voc_size,\n",
        "            emb_dim=emb_dim,\n",
        "            model_dim=model_dim)\n",
        "ckpt = tf.train.Checkpoint(model)\n",
        "ckpt_manager = tf.train.CheckpointManager(\n",
        "    ckpt, \n",
        "    directory=\"./rnnModelCheckpoint/\", \n",
        "    max_to_keep=3\n",
        ")\n",
        "ckpt.restore(ckpt_manager.latest_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-TrUjpIvkWZ"
      },
      "source": [
        "### Generación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjzI_h9JvkWa"
      },
      "outputs": [],
      "source": [
        "def generate(start):\n",
        "    \"\"\"\n",
        "    Dada una palabra de inicio genera el texto que le sigue\n",
        "    \"\"\"\n",
        "    states = None\n",
        "    context = tf.constant([start])\n",
        "    output = [start]\n",
        "    for i in range(21):\n",
        "        # Obtener solo el primer elemento que regresa vectorize_layer\n",
        "        pred_logits, states = model(vectorize_layer(context)[:, :1], \n",
        "                                    states=states, return_state=True)\n",
        "        #print(pred_logits.shape)\n",
        "        pred_index = tf.random.categorical(pred_logits[:, -1, :], \n",
        "                                        num_samples=1)\n",
        "\n",
        "        #print(vocab[pred_index[0, 0]])\n",
        "        context = tf.constant([vocab[pred_index[0, 0]]])\n",
        "        output.append(vocab[pred_index[0, 0]])\n",
        "    return output\n",
        "    \n",
        "' '.join(generate('Dr Strange'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generando oraciones para ser evaluadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = pickle.load(open('./pickles/datasets/test.pkl','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictRNN = []\n",
        "\n",
        "for sentence in tqdm(test):\n",
        "    sentence_splited = sentence.split(' ')[:]\n",
        "    length = len(sentence_splited)\n",
        "    half_index = length // 2\n",
        "    half = ' '.join(sentence_splited[:half_index])\n",
        "\n",
        "    predictRNN.append(' '.join(generate(half)))\n",
        "\n",
        "predictRNN[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpiamos oraciones generadas de espacios y oraciones vacías\n",
        "predictRNN = list(map(lambda x: x.strip(), predictRNN))\n",
        "predictRNN = list(filter(lambda x: len(x) > 0, predictRNN))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Guardamos para evaluar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle.dump(predictRNN, open('./pickles/predict/rnn.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ProyectoFinal.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "602bbe8e8fe3127dc2d2bb8f57c49cfbb507be2cc0e090f10eb4c4d0af354b8b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
